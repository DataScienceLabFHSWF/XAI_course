\chapter*{Exercise 6}
\section*{Explainable AI: LLM Interpretability with Llama2}

\section*{Exercise 1: Setup \& Baseline Attribution (30 mins)}
\textbf{Tools:} Captum, HuggingFace Transformers

\begin{enumerate}
\item Load a pretrained LLM with huggingface that fits your GPU
\item if no GPU is available use the \url{https://www.ki.fh-swf.de/} server
\item Llama-3.1-8B \url{https://huggingface.co/meta-llama/Llama-3.1-8B}
\item Llama-3.2-1B \url{https://huggingface.co/meta-llama/Llama-3.2-1B}
\item Llama-3.2-1B \url{https://huggingface.co/meta-llama/Llama-3.2-3B}
\begin{minted}{python}
from transformers import AutoTokenizer, AutoModelForCausalLM
model = AutoModelForCausalLM.from_pretrained("/meta-llama/Llama-3.1-8B")
tokenizer = AutoTokenizer.from_pretrained("/meta-llama/Llama-3.1-8B")
\end{minted}

\item Generate baseline predictions
\begin{minted}{python}
prompt = "Explain the concept of quantum entanglement:"
inputs = tokenizer(prompt, return_tensors="pt")
outputs = model.generate(inputs.input_ids, max_length=200)
print(tokenizer.decode(outputs[0]))
\end{minted}
\end{enumerate}

\section*{Exercise 2: Feature Attribution (45 mins)}
\textbf{Method:} Layer Integrated Gradients

\begin{enumerate}
\item Implement attribution calculation
\begin{minted}{python}
from captum.attr import LayerIntegratedGradients

def attribute(model, inputs):
    lig = LayerIntegratedGradients(model, model.model.layers[0])
    attributions = lig.attribute(
        inputs=inputs.input_ids,
        baselines=tokenizer("", return_tensors="pt").input_ids,
        target=outputs
    )
    return attributions
\end{minted}

\item Visualize token importance
\begin{minted}{python}
import numpy as np
import matplotlib.pyplot as plt

tokens = tokenizer.convert_ids_to_tokens(inputs.input_ids[0])
attr_scores = np.sum(attributions[0].detach().numpy(), axis=1)

plt.barh(tokens, attr_scores)
plt.title("Token Attribution Scores")
plt.show()
\end{minted}
\end{enumerate}

\section*{Exercise 3: Comparative Analysis (15 mins)}
\begin{itemize}
\item Compare results with different baselines:
\begin{minted}{python}
baselines = [
    tokenizer(" ", return_tensors="pt").input_ids,  # Single space
    tokenizer("<unk>", return_tensors="pt").input_ids  # Unknown token
]
\end{minted}
\item Discuss stability of attributions
\end{itemize}

\section*{Theoretical Questions}
\begin{enumerate}
\item Why use layer-specific attribution instead of full-model?
\item How does choice of baseline impact IG results?
\item What challenges arise when interpreting auto-regressive models?
\end{enumerate}

\section*{Optimization Tips}
\begin{itemize}
\item Use \texttt{model.half()} for FP16 precision
\item Limit sequence length to 512 tokens
\item Cache model weights with \texttt{model.cache=True}
\item Use \texttt{return\_convergence\_delta=True} for reliability checks
\end{itemize}

\section*{Advanced Tasks (Bonus)}
\begin{itemize}
\item Implement attention head visualization
\item Compare with \texttt{FeatureAblation} method
\item Calculate Quantus metrics for attribution consistency
\end{itemize}

\section*{Recommended Resources}
\begin{itemize}
\item \href{https://captum.ai/tutorials/Llama2_LLM_Attribution}{Official Captum Tutorial}
\item \href{https://arxiv.org/abs/2307.03381}{Llama2 Paper}
\item \href{https://github.com/facebookresearch/llama}{Llama2 GitHub}
\end{itemize}
