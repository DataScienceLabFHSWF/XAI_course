\chapter*{Exercise Sheet 5}
\section*{Exercise 1: Basic Attention Visualization}
\textbf{Tools:} BERTviz \& HuggingFace Transformers

\begin{enumerate}
\item Setup environment and load model
\begin{minted}{bash}
pip install bertviz transformers
\end{minted}

\item Visualize attention in different layers
\begin{minted}{python}
from bertviz import head_view
from transformers import BertModel, BertTokenizer

model = BertModel.from_pretrained('bert-base-uncased', output_attentions=True)
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

text = "The cat sat on the mat because it was tired."
inputs = tokenizer.encode(text, return_tensors='pt')
outputs = model(inputs)
attention = outputs[-1]  # Tuple of attention tensors

# Visualize
head_view(attention, tokenizer.convert_ids_to_tokens(inputs[0]))
\end{minted}

\item Tasks:
\begin{itemize}
\item Identify attention heads focusing on pronouns ("it")
\item Compare patterns in early vs. final layers
\item Find heads specializing in syntactic vs semantic relationships
\end{itemize}
\end{enumerate}

\section*{Exercise 2: Feature Attribution with Captum }
\textbf{Resources:} \href{https://captum.ai/tutorials/Bert_SQUAD_Interpret}{Captum BERT Tutorial}

\begin{enumerate}
\item Implement integrated gradients
\begin{minted}{python}
from captum.attr import IntegratedGradients

ig = IntegratedGradients(model)
attributions = ig.attribute(inputs, target=0, 
                           n_steps=50, 
                           return_convergence_delta=True)
\end{minted}

\item Compare with attention weights
\begin{minted}{python}
# Visualize both side-by-side
import matplotlib.pyplot as plt
fig, (ax1, ax2) = plt.subplots(1, 2)
ax1.imshow(attention[0][0].detach().numpy())
ax2.imshow(attributions[0].sum(dim=-1).detach().numpy())
\end{minted}
\end{enumerate}

\section*{Theoretical Questions}
\begin{itemize}
\item How does attention visualization differ from feature attribution?
\item What are the limitations of visualizing attention as explanation?
\item When would you prefer attention visualization over gradient-based methods?
\end{itemize}

\section*{Advanced Tasks (Bonus)}
\begin{itemize}
\item Modify visualization for multi-head comparison
\item Calculate attention head importance scores
\item Implement custom attention pattern filtering
\end{itemize}

\section*{Troubleshooting Tips}
\begin{itemize}
\item If BERTviz doesn't render: Use Jupyter notebook directly
\item For CUDA memory issues: Reduce model size to `bert-tiny`
\item Version conflicts: Pin to `transformers==4.25.1`, `bertviz==1.0.0`
\end{itemize}

\section*{Recommended Resources}
\begin{itemize}
\item \href{https://github.com/jessevig/bertviz}{BERTviz GitHub}
\item \href{https://captum.ai/tutorials/Bert_SQUAD_Interpret}{Captum BERT Tutorial}
\item \href{https://arxiv.org/abs/2004.10102}{Attention is not Explanation (Paper)}
\end{itemize}